{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48450fd1",
   "metadata": {},
   "source": [
    "# Model Pruning and Regularization\n",
    "\n",
    "Controlling model complexity through depth constraints, feature reduction, and regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b764559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5922aa",
   "metadata": {},
   "source": [
    "## Regularization and Structural Pruning Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e1a2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"rmse\",\n",
    "    n_estimators=300,        # fewer trees\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,             # was 6\n",
    "    min_child_weight=5,      # minimum samples per leaf\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.7,    # less feature sampling -> less variance\n",
    "    reg_lambda=1.0,          # L2 regularization\n",
    "    reg_alpha=0.5,           # L1 regularization\n",
    "    tree_method=\"hist\",\n",
    "    random_state=2025\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a96dea6",
   "metadata": {},
   "source": [
    "## Evaluation Framework\n",
    "To consistently compare pruned and regularized model variants, a unified evaluation function was implemented to report RMSE, MAE, and R² on both training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108070b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(pipeline, X_train, X_test, y_train, y_test):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Training predictions\n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "    rmse_train = (mean_squared_error(y_train, y_pred_train)) ** 0.5\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "\n",
    "    # Testing predictions\n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "    rmse_test = (mean_squared_error(y_test, y_pred_test)) ** 0.5\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "    print(\"=== TRAINING RESULTS ===\")\n",
    "    print(f\"RMSE Train: {rmse_train:,.2f}\")\n",
    "    print(f\"MAE Train:  {mae_train:,.2f}\")\n",
    "    print(f\"R² Train:   {r2_train:.3f}\")\n",
    "    print(\"\")\n",
    "    print(\"=== TESTING RESULTS ===\")\n",
    "    print(f\"RMSE Test: {rmse_test:,.2f}\")\n",
    "    print(f\"MAE Test:  {mae_test:,.2f}\")\n",
    "    print(f\"R² Test:   {r2_test:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"rmse_train\": rmse_train, \"rmse_test\": rmse_test,\n",
    "        \"mae_train\": mae_train, \"mae_test\": mae_test,\n",
    "        \"r2_train\": r2_train, \"r2_test\": r2_test\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebfed92",
   "metadata": {},
   "source": [
    "## Option A – Pruned and Regularized Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec22c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_reg_A = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"rmse\",\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    min_child_weight=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_lambda=1.0,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method=\"hist\",\n",
    "    random_state=2025\n",
    ")\n",
    "\n",
    "pipeline_A = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", xgb_reg_A),\n",
    "])\n",
    "\n",
    "print(\"===== OPTION A RESULTS =====\")\n",
    "results_A = evaluate_model(pipeline_A, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c17d0f0",
   "metadata": {},
   "source": [
    "## Option B – Reduced Feature Set + Pruned Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2760da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reduced feature set\n",
    "### Reduced Feature Set\n",
    "reduced_features = [\n",
    "    \"PTS\", \"MP\", \"Age\",\n",
    "    \"AST\", \"TRB\",\n",
    "    \"FT\", \"FTA\",\n",
    "    \"STL\", \"BLK\",\n",
    "    \"G\", \"GS\",\n",
    "    \"Year\", \"Team\"\n",
    "]\n",
    "\n",
    "X_red = df_reg[reduced_features]\n",
    "y_red = df_reg[\"Salary\"]\n",
    "\n",
    "# 2. Train/test split for Option B\n",
    "X_train_B, X_test_B, y_train_B, y_test_B = train_test_split(\n",
    "    X_red, y_red, test_size=0.2, random_state=2025\n",
    ")\n",
    "\n",
    "# 3. NEW preprocessor for Option B (based ONLY on X_red)\n",
    "numeric_features_B = X_red.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_features_B = X_red.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "numeric_transformer_B = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer_B = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor_B = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer_B, numeric_features_B),\n",
    "        (\"cat\", categorical_transformer_B, categorical_features_B),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. Use the same tuned XGBoost params from Option A\n",
    "xgb_reg_A = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"rmse\",\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    min_child_weight=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_lambda=1.0,\n",
    "    reg_alpha=0.5,\n",
    "    tree_method=\"hist\",\n",
    "    random_state=2025\n",
    ")\n",
    "\n",
    "pipeline_B = Pipeline([\n",
    "    (\"preprocess\", preprocessor_B),\n",
    "    (\"model\", xgb_reg_A),\n",
    "])\n",
    "\n",
    "print(\"===== OPTION B RESULTS =====\")\n",
    "results_B = evaluate_model(pipeline_B, X_train_B, X_test_B, y_train_B, y_test_B)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
