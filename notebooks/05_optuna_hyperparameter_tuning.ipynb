{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32018ed",
   "metadata": {},
   "source": [
    "# Optuna Hyperparameter Tuning – Pruned XGBoost Model\n",
    "\n",
    "This notebook performs automated hyperparameter tuning using Optuna on the pruned and regularized feature set derived in the previous notebook.\n",
    "\n",
    "Objective: Minimize cross-validated RMSE on the training window (2019–2024).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af933ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libaries\n",
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a025c0",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline for Reduced Feature Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77074bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric & categorical columns for this reduced feature set\n",
    "numeric_features_B = X_red.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_features_B = X_red.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "def create_preprocessor_B():\n",
    "    numeric_transformer_B = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer_B = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    preprocessor_B = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer_B, numeric_features_B),\n",
    "            (\"cat\", categorical_transformer_B, categorical_features_B),\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor_B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87580835",
   "metadata": {},
   "source": [
    "## Optuna Objective Function (3-Fold Cross-Validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb25d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # ----- Hyperparameter search space -----\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 800),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 5.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 5.0),\n",
    "    }\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        eval_metric=\"rmse\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=2025,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    preprocessor_B = create_preprocessor_B()\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"preprocess\", preprocessor_B),\n",
    "        (\"model\", xgb_model),\n",
    "    ])\n",
    "\n",
    "    # ----- 3-fold cross-validation -----\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=2025)\n",
    "\n",
    "    rmses = []\n",
    "    for train_idx, valid_idx in kf.split(X_red):\n",
    "        X_train_cv, X_valid_cv = X_red.iloc[train_idx], X_red.iloc[valid_idx]\n",
    "        y_train_cv, y_valid_cv = y_red.iloc[train_idx], y_red.iloc[valid_idx]\n",
    "\n",
    "        pipe.fit(X_train_cv, y_train_cv)\n",
    "        y_pred_cv = pipe.predict(X_valid_cv)\n",
    "\n",
    "        mse = mean_squared_error(y_valid_cv, y_pred_cv)\n",
    "        rmse = np.sqrt(mse)\n",
    "        rmses.append(rmse)\n",
    "\n",
    "    # Optuna will MINIMIZE the average RMSE\n",
    "    return np.mean(rmses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5222f000",
   "metadata": {},
   "source": [
    "## Run Optuna Study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e094ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=700, show_progress_bar=True)  # you can increase n_trials later\n",
    "\n",
    "print(\"Best RMSE:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbd0f43",
   "metadata": {},
   "source": [
    "## Final Optimized Model Construction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a9124",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "print(best_params)\n",
    "\n",
    "best_xgb = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"rmse\",\n",
    "    tree_method=\"hist\",\n",
    "    random_state=2025,\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "preprocessor_B = create_preprocessor_B()\n",
    "\n",
    "best_pipeline = Pipeline([\n",
    "    (\"preprocess\", preprocessor_B),\n",
    "    (\"model\", best_xgb),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9831d6af",
   "metadata": {},
   "source": [
    "## Optimized Pipeline Ready for Final Training\n",
    "\n",
    "The pipeline constructed above will be used in the next notebook to train the final model and evaluate performance on the holdout test set.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
